{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "cb4fba54-0a77-491e-86aa-527d03227c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "5a779675-e858-46fc-9c47-39204da6b591",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('fake_job_postings.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "7dc3c41c-125a-4392-a21f-5aa5420d9b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------1. split the columns to country, state, and city\n",
    "\n",
    "# to use country only?\n",
    "df['country']=df.location.str.split(',',expand=True)[:][0]\n",
    "\n",
    "df['state']=df.location.str.split(',',expand=True)[:][1]\n",
    "df['city']=df.location.str.split(',',expand=True)[:][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "e1475422-6826-48c9-8b60-bc852c0dd411",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#----------2. replacing different kinds of missing value to np.nan\n",
    "df.state = df.state.str.strip()\n",
    "df.state.fillna(value=np.nan, inplace=True)\n",
    "df.state.replace('', np.nan, inplace=True)\n",
    "df.state.replace(' ', np.nan, inplace=True)\n",
    "\n",
    "\n",
    "#df['state'].sort_values().unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "cc386e73-2d80-4260-9021-96fd883fdf3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------3. clear the whitespaces and signs at the start/end\n",
    "\n",
    "df.city = df.city.str.strip(' /:\\\\')\n",
    "\n",
    "#---------4. replacing different kinds of missing value to np.nan\n",
    "df.city.fillna(value=np.nan, inplace=True)\n",
    "df.city.replace('', np.nan, inplace=True)\n",
    "df.city.replace(' ', np.nan, inplace=True)\n",
    "df['city']=df.city.str.lower()\n",
    "#------\n",
    "\n",
    "#print(df['city'].sort_values().unique().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "5f0af630-5154-4bbf-821d-86f68bd4cdf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.salary_range.fillna(value=np.nan, inplace=True)\n",
    "df.salary_range.replace('', np.nan, inplace=True)\n",
    "df.salary_range.replace(' ', np.nan, inplace=True)\n",
    "\n",
    "#---------4. spilt salary range into min and max\n",
    "\n",
    "df['min_salary']=df.salary_range.str.split('-',expand=True)[:][0]\n",
    "df['max_salary']=df.salary_range.str.split('-',expand=True)[:][1]\n",
    "\n",
    "df.max_salary.fillna(value=np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "85f9c310-615c-45c5-a410-0f40f292add8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------5. for entry of salary_range as date, max and min salary are grouped as null value\n",
    "df.loc[df['max_salary'].isin(['Apr', 'Dec', 'Jun', 'Nov', 'Oct', 'Sep']),['max_salary', 'min_salary']]=np.nan\n",
    "df.loc[df['min_salary'].isin(['Dec', 'Jun', 'Oct']),['max_salary', 'min_salary']]=np.nan\n",
    "\n",
    "#convert them into numerical value\n",
    "df[['min_salary','max_salary']] = df[['min_salary','max_salary']].astype(float)\n",
    "\n",
    "#for regression model, need to impute NaN values to median/mean\n",
    "# df['max_salary'].fillna(value=df['max_salary'].mean(), inplace=True)\n",
    "# df['min_salary'].fillna(value=df['min_salary'].mean(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e73c8c-1414-43cc-a533-c8fbe3abc3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------end of processing for column salary_range and location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8760c843",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------start cleaning for text columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79e4f20-66a5-4fc6-bd16-a7a6b1ef604e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function for preprocessing text is used linked from the next function\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Tokenise words while ignoring punctuation\n",
    "    tokeniser = RegexpTokenizer(r'(?u)\\b\\w\\w+\\b')\n",
    "    tokens = tokeniser.tokenize(text)\n",
    "    \n",
    "    # Lowercase and lemmatise \n",
    "    lemmatiser = WordNetLemmatizer()\n",
    "    lemmas = [lemmatiser.lemmatize(token.lower(), pos='v') for token in tokens]\n",
    "    \n",
    "    # Remove stopwords\n",
    "    keywords= [lemma for lemma in lemmas if lemma not in stopwords.words('english')]\n",
    "    return keywords\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5ee96a-6ca6-4d83-9391-8299e7c25977",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorise_column(df, column):\n",
    "    # Fill empty columns with \"Unspecified\"\n",
    "    df[column] = df[column].fillna(\"Unspecified\")\n",
    "    \n",
    "    # Create an instance of TfidfVectorizer\n",
    "    vectoriser = TfidfVectorizer(analyzer=preprocess_text, ngram_range = (1,2))\n",
    "\n",
    "    # Fit to the data and transform to feature matrix\n",
    "    text_column = vectoriser.fit_transform(df[column])\n",
    "\n",
    "    # Convert sparse matrix to dataframe\n",
    "    text_column = pd.DataFrame.sparse.from_spmatrix(text_column)\n",
    "\n",
    "    # Save mapping on which index refers to which words\n",
    "    col_map = {v:k for k, v in vectoriser.vocabulary_.items()}\n",
    "\n",
    "    # Rename each column using the mapping\n",
    "    for col in text_column.columns:\n",
    "        text_column.rename(columns={col: col_map[col]}, inplace=True)\n",
    "    \n",
    "    # Combined to dataframe\n",
    "    combined = [df, text_column]\n",
    "    df = pd.concat(combined, axis =1)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d054c6-718f-4118-a435-66c02655f345",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new table for text which can be merged to main table later\n",
    "\n",
    "df_text = df.loc[:, ['company_profile', 'description', 'requirements', 'benefits']]\n",
    "df_text['text'] = df_text.apply(lambda row: (str(row['company_profile']) + ' ' + str(row['description'])  \n",
    "                                            + ' ' + str(row['requirements']) + ' ' + str(row['benefits'])), axis = 1)\n",
    "to_merge = vectorise_column(df_text, 'text')\n",
    "\n",
    "# Only the following columns need to be merged\n",
    "# to_merge.iloc[:,4:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b007fc4d-73fe-4b13-b5f0-5b99ab250be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------end cleaning for text columns"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
